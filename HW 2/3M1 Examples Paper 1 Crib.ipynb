{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3M1 Examples Paper crib (linear algebra)\n",
    "\n",
    "\n",
    "Garth N. Wells (gnw20@cam.ac.uk)\n",
    "\n",
    "January 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "An LU decomposition of\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{A} = \\begin{bmatrix}\n",
    "2  & 4  & -2 & 2\\\\\n",
    "1  & 3  & 0  & 2\\\\\n",
    "-1 & -1 & 2  & 0\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "is\n",
    "\n",
    "\\begin{equation}\n",
    "  \\boldsymbol{A} =\n",
    "    \\begin{bmatrix}\n",
    "      1    & 0  & 0\\\\\n",
    "      0.5  & 1  & 0\\\\\n",
    "      -0.5 & 1  & 1\n",
    "    \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "      2  & 4  & -2 & 2\\\\\n",
    "      0  & 1  & 1  & 1\\\\\n",
    "      0  & 0  & 0  & 0\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $r =  \\text{rank}(\\boldsymbol{A}) = \\text{rank}(\\boldsymbol{U}) =2$ (number of nonzero \n",
    "  rows in $\\boldsymbol{U}$).\n",
    "\n",
    "- Column space spanned by the first $r$ columns of $\\boldsymbol{L}$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{bmatrix}\n",
    "    1    \\\\ 0.5  \\\\ -0.5\n",
    "    \\end{bmatrix}, \\\n",
    "    \\begin{bmatrix}\n",
    "    0 \\\\ 1 \\\\ 1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Row space spanned by the nonzero rows of $\\boldsymbol{U}$:\n",
    "\n",
    "\\begin{equation}\n",
    "   \\begin{bmatrix}\n",
    "   2    \\\\ 4  \\\\ -2 \\\\ 2\n",
    "   \\end{bmatrix}, \\\n",
    "   \\begin{bmatrix}\n",
    "   0    \\\\ 1  \\\\ 1 \\\\ 1\n",
    "   \\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nullspace contains all vectors that satisfy $\\boldsymbol{U} \\boldsymbol{x} = \n",
    "  \\boldsymbol{0}$. For $\\boldsymbol{x} = \\begin{bmatrix} x_{1} & x_{2} & x_{3} &   \n",
    "  x_{4}\\end{bmatrix}$, we get\n",
    "\n",
    "\\begin{align*}\n",
    "  2x_{1} + 4x_{2} - 2x_{3} + 2x_{4} &= 0\\\\\n",
    "  x_{2} + x_{3} + x_{4} &= 0\n",
    "\\end{align*}\n",
    "\n",
    "  from which $x_{2} = -x_{3} - x_{4}$ and  \n",
    "  $x_{1} = (1/2)[-4(-x_{3} - x_{4}) +2x_{3} - 2x_{4}] = 3x_{3} + x_{4}$. Therefore\n",
    "\n",
    "\\begin{equation}\n",
    "  \\boldsymbol{x} =\n",
    "   x_{3} \\begin{bmatrix}\n",
    "   3    \\\\ -1  \\\\ 1 \\\\ 0\n",
    "   \\end{bmatrix}\n",
    "   +\n",
    "   x_{4} \\begin{bmatrix}\n",
    "   1    \\\\ -1  \\\\ 0 \\\\ 1\n",
    "   \\end{bmatrix}\n",
    "\\end{equation}\n",
    "   \n",
    "  The two vectors span the nullspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The left-nullspace is the nullspace of $\\boldsymbol{A}^{T}$, and is orthogonal to the \n",
    "  column space of $\\boldsymbol{A}$. We can deduce then from the column space that the    \n",
    "  left-nullspace is spanned by\n",
    "\n",
    "\\begin{equation}\n",
    "   x_{3} \\begin{bmatrix}\n",
    "   1    \\\\ -1  \\\\ 1\n",
    "   \\end{bmatrix}.\n",
    "\\end{equation}\n",
    "  \n",
    "  This vector is orthogonal to the two vectors in the column space.\n",
    "  \n",
    "We can check the LU decompistion with NumPy/Scipy. We first enter the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A matrix: \n",
      " [[ 2  4 -2  2]\n",
      " [ 1  3  0  2]\n",
      " [-1 -1  2  0]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[2, 4, -2, 2], [1, 3, 0, 2], [-1, -1, 2, 0]])\n",
    "print(\"A matrix: \\n\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the compute the LU factorisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L = \n",
      " [[ 1.   0.   0. ]\n",
      " [ 0.5  1.   0. ]\n",
      " [-0.5  1.   1. ]]\n",
      "U = \n",
      " [[ 2.  4. -2.  2.]\n",
      " [ 0.  1.  1.  1.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "P, L, U = linalg.lu(A)\n",
    "print(\"L = \\n\", L)\n",
    "print(\"U = \\n\", U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "To show that $\\boldsymbol{A}^{T} \\boldsymbol{A}$ is positive semi-definite:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\boldsymbol{x}^{T} \\boldsymbol{A}^{T} \\boldsymbol{A} \\boldsymbol{x} \n",
    "  =\n",
    "  (\\boldsymbol{A}\\boldsymbol{x})^{T} (\\boldsymbol{A} \\boldsymbol{x})\n",
    "  =\n",
    "  (\\boldsymbol{A}\\boldsymbol{x}) \\cdot (\\boldsymbol{A} \\boldsymbol{x})\n",
    "  \\ge 0\n",
    "\\end{equation}\n",
    "\n",
    "If $(\\lambda, \\boldsymbol{x})$ is an eigenpair of $\\boldsymbol{A}^{T} \\boldsymbol{A}$,\n",
    "\n",
    "\\begin{equation}\n",
    "  \\boldsymbol{A}^{T} \\boldsymbol{A} \\boldsymbol{x} \n",
    "  = \\lambda \\boldsymbol{x}\n",
    "\\end{equation}\n",
    "\n",
    "Premultiplying both sides by $\\boldsymbol{x}^{T}$:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\boldsymbol{x}^{T}  \\boldsymbol{A}^{T} \\boldsymbol{A} \\boldsymbol{x} \n",
    "  = \\lambda \\boldsymbol{x}^{T} \\boldsymbol{x}\n",
    "\\end{equation}\n",
    "\n",
    "Since $\\boldsymbol{x}^{T}  \\boldsymbol{A}^{T} \\boldsymbol{A} \\boldsymbol{x} \\ge 0$ and $\\boldsymbol{x}^{T} \\boldsymbol{x} \\ge 0$, $\\lambda$ must be greater then or equal to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "a) Eigenvalues of $\\boldsymbol{A}$ satisfy $\\det(\\boldsymbol{A} - \\lambda\\boldsymbol{I}) \n",
    "  = 0$. For the transpose, we have \n",
    "\n",
    "\\begin{equation}\n",
    "\\det(\\boldsymbol{A}^{T} - \\lambda\\boldsymbol{I}) = \\det( (\\boldsymbol{A} - \\lambda    \\boldsymbol{I})^{T}) = \\det(\\boldsymbol{A} - \\lambda \\boldsymbol{I}),\n",
    "\\end{equation}\n",
    "\n",
    "  since the $\\det\\boldsymbol{A} = \\det \\boldsymbol{A}^{T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) If $\\lambda$ and $\\boldsymbol{x}$ are an eigenvalue and eigenvector, respectively of $\\boldsymbol{A}\\boldsymbol{B}$, then\n",
    "\n",
    "\\begin{equation}\n",
    "  \\boldsymbol{A}\\boldsymbol{B} \\boldsymbol{x} = \\lambda \\boldsymbol{x}\n",
    "\\end{equation}\n",
    "\n",
    "Multiplying both sides by $\\boldsymbol{B}$, \n",
    "\n",
    "\\begin{equation}\n",
    "  (\\boldsymbol{B}\\boldsymbol{A}) (\\boldsymbol{B} \\boldsymbol{x}) \n",
    "  = \\lambda (\\boldsymbol{B}\\boldsymbol{x})\n",
    "\\end{equation}\n",
    "\n",
    "We see that $\\lambda$ is an eigenvalue of $\\boldsymbol{B}\\boldsymbol{A}$, and the eigenvector is now $\\boldsymbol{B} \\boldsymbol{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Follows from the definition of the norms: \n",
    "\n",
    "\\begin{equation}\n",
    "  \\max_{i=1}^{n} |x_{i}| \\le \\sum_{i=1}^{n} |x_{i}| \\le n \\max_{i=1}^{n} |x_{i}|.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Recall that from the defintion of a matrix operator norm is follows that $\\| \\boldsymbol{A}\\boldsymbol{x} \\| \\le \\| \\boldsymbol{A} \\| \\| \\boldsymbol{x} \\|$ \n",
    "\n",
    "a) From the definition of the matrix norm:\n",
    "\n",
    "\\begin{equation}\n",
    "   \\| \\boldsymbol{A} \\boldsymbol{B} \\boldsymbol{x} \\| \n",
    "   \\le \\| \\boldsymbol{A} \\| \\| \\boldsymbol{B} \\boldsymbol{x}\\| \n",
    "   \\le \\| \\boldsymbol{A} \\| \\| \\boldsymbol{B}\\| \\| \\boldsymbol{x}\\|\n",
    "\\end{equation}\n",
    "   \n",
    "   for all $\\boldsymbol{x}$. Re-arranging,\n",
    "\n",
    "\\begin{equation}\n",
    "   \\frac{\\| \\boldsymbol{A} \\boldsymbol{B} \\boldsymbol{x} \\|}{\\|\\boldsymbol{x}\\|} \n",
    "   \\le \\| \\boldsymbol{A} \\| \\| \\boldsymbol{B}\\|\n",
    "\\end{equation}\n",
    "\n",
    "   for all $\\boldsymbol{x} \\ne \\boldsymbol{0}$. From the definition of the norm, $\\|    \n",
    "   \\boldsymbol{A} \\boldsymbol{B} \\|$ is the largest possible value of $\\| \\boldsymbol{A} \n",
    "   \\boldsymbol{B} \\boldsymbol{x} \\| / \\|\\boldsymbol{x}\\|$ (over all\n",
    "   $\\boldsymbol{x}$, exluding the zero vector), hence\n",
    "\n",
    "\\begin{equation}\n",
    "    \\| \\boldsymbol{A} \\boldsymbol{B} \\|  \\le \\| \\boldsymbol{A} \\| \\| \\boldsymbol{B}\\|\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)\n",
    "\\begin{equation}\n",
    "   \\| (\\boldsymbol{A} + \\boldsymbol{B}) \\boldsymbol{x} \\| \n",
    "   = \\| \\boldsymbol{A}\\boldsymbol{x} + \\boldsymbol{B} \\boldsymbol{x} \\| \n",
    "\\end{equation}\n",
    "\n",
    "   From the triagle inequality for vectors,\n",
    "\n",
    "\\begin{align}\n",
    "   \\| \\boldsymbol{A}\\boldsymbol{x} + \\boldsymbol{B} \\boldsymbol{x} \\| \n",
    "   &\\le\n",
    "   \\| \\boldsymbol{A}\\boldsymbol{x} \\| + \\| \\boldsymbol{B} \\boldsymbol{x} \\|  \\\\\n",
    "   &\\le\n",
    "   \\| \\boldsymbol{A} \\| \\| \\boldsymbol{x} \\| + \\| \\boldsymbol{B} \\| |\\boldsymbol{x} \\| \n",
    "\\end{align}\n",
    "\n",
    "   Re-arranging\n",
    "\n",
    "\\begin{equation}\n",
    "   \\frac{\\| (\\boldsymbol{A} + \\boldsymbol{B}) \\boldsymbol{x} \\|}{\\| \\boldsymbol{x} \\|}\n",
    "   \\le\n",
    "   \\| \\boldsymbol{A} \\| + \\| \\boldsymbol{B} \\|  \n",
    "\\end{equation}\n",
    "\n",
    "   Using same argument as from part a), we get $\\| \\boldsymbol{A} + \\boldsymbol{B} \\| \\le  \n",
    "   \\| \\boldsymbol{A} \\| + \\| \\boldsymbol{B} \\|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Recall that the smallest possible value of $C$ for which $\\|{\\boldsymbol{A}\\boldsymbol{x}}\\| \\le C \\| \\boldsymbol{x}\\|$ holds is the matrix norm $\\|\\boldsymbol{A}\\|$. The task is to find $C$ for the different norms. It simplifies the proofs if we consider all vectors for which $\\|\\boldsymbol{x}\\| = 1$ (this is possible since $\\|\\alpha \\boldsymbol{x}\\| = |\\alpha| \\|\\boldsymbol{x}\\|$), in which case the task is to find the smallest possible $C$ such that $\\|\\boldsymbol{A}\\boldsymbol{x}\\| \\le C$.\n",
    "\n",
    "a) For the 1-norm, denoting the $j$th column of $\\boldsymbol{A}$ by\n",
    "   $\\boldsymbol{a}_{j}$, and for a vector $\\|\\boldsymbol{x}\\|_{1} = 1$:\n",
    "\n",
    "\\begin{align*}\n",
    "        \\| \\boldsymbol{A} \\boldsymbol{x}\\|_{1}\n",
    "        = \\|\\sum_{j=1}^{n} \\boldsymbol{a}_{j} x_{j}\\|_{1}\n",
    "        \\le\n",
    "        \\sum_{j=1}^{n} \\|\\boldsymbol{a}_{j}\\|_{1} |x_{j}|\n",
    "        \\le \\max_{j=1}^{n} \\|\\boldsymbol{a}_{j}\\|_{1}\n",
    "\\end{align*}\n",
    "\n",
    "   This is the maximum column sum.\n",
    "\n",
    "   The term on the right could possibly be larger than $\\| \\boldsymbol{A} \\|_{1}$, whereas  \n",
    "   the norm is the smallest possible value for the RHS that still satisfies the\n",
    "   inequalities.  If we can show a case for \n",
    "   which equality is reached, $\\|\\boldsymbol{A} \\boldsymbol{x}\\|_{1} = \\max_{j=1}^{n}\n",
    "   \\|\\boldsymbol{a}_{j}\\|_{1}$, we have the norm, \n",
    "   For vector the $\\boldsymbol{e}_{j}$ with $e_{j} = 1$, \n",
    "   where $j$ is the column with the greatest 1-norm, and $e_{i \\ne j} = 0$, we have\n",
    "\n",
    "\\begin{equation}\n",
    "     \\|\\boldsymbol{A} \\boldsymbol{e}_{j}\\|_{1} = \\max_{j=1}^{n} \\|\\boldsymbol{a}_{j}\\|_{1}\n",
    "\\end{equation}\n",
    "\n",
    "   Therefore,\n",
    "\n",
    "\\begin{equation}\n",
    "      \\|\\boldsymbol{A}\\|_{1} = \\max_{j=1}^{n} \\|\\boldsymbol{a}_{j}\\|_{1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) For a vector $\\|\\boldsymbol{x}\\|_{\\infty} = 1$:\n",
    "\n",
    "\\begin{align*}\n",
    "       \\|\\boldsymbol{A} \\boldsymbol{x}\\|_{\\infty}\n",
    "          = \\max_{i=1}^{m} |\\sum_{j=1}^{n} a_{ij} x_{j}|\n",
    "          &\\le \\max_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}| |x_{j}|\n",
    "          \\\\\n",
    "          &\\le \\max_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|\n",
    "\\end{align*}\n",
    "\n",
    "   The RHS is the maximum row sum.\n",
    "\n",
    "   As before, we need to find a case with equality.  If the row\n",
    "   with the maximum sum is row $k$, we choose a vector $\\boldsymbol{x}$\n",
    "   where $x_{i} = \\pm 1$ such that the sign of $x_{i}$ is the\n",
    "   same as the sign of the entry $a_{ki}$. We then have\n",
    "\n",
    "\\begin{equation}\n",
    "      \\|\\boldsymbol{A}\\|_{\\infty}\n",
    "      = \\max_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "Recall that the $l_{2}$ norm is the square root of the maximum eigenvalue of $\\boldsymbol{A}^{T}\\boldsymbol{A}$. We have\n",
    "\n",
    "$$\n",
    "    \\boldsymbol{A}^{T}\\boldsymbol{A}\n",
    "    =\n",
    "     \\begin{bmatrix}\n",
    "       26 & 5 \\\\ 5 & 1\n",
    "     \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For this matrix, $\\lambda = (27 \\pm \\sqrt{27^{2} -4})/2$. \n",
    "Hence $\\|\\boldsymbol{A}\\|_{2} \\approx 5.1926$.\n",
    "\n",
    "We have\n",
    "\n",
    "$$\n",
    "    \\boldsymbol{A}\n",
    "    \\begin{bmatrix} x_{1} \\\\ x_{2}  \\end{bmatrix}\n",
    "    =    \n",
    "    \\begin{bmatrix} x_{1} \\\\ 5x_{1} + x_{2}  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Consider the case where the above vector has unit length ($\\|\\boldsymbol{x}\\|_{2} = 1$), hence we set $x_{2} = \\sqrt{1 - x_{1}^{2}}$. We then have\n",
    "\n",
    "\n",
    "$$\n",
    "  F(x_{1}) = \\|\\boldsymbol{A}\\boldsymbol{x}\\|_{2}^{2}\n",
    "    = 25 x_{1}^{2} + 10 x_{1} \\sqrt{1 - x_{1}^{2}} + 1\n",
    "$$\n",
    "\n",
    "To find the extreme points of the function, we differentiate $F$ with respect to $x_{1}$ \n",
    "and set the derivative equal to zero. This yields a maximum of\n",
    "\n",
    "$$\n",
    " x_{1, \\max} = \\frac{29 + \\sqrt{29^{2} - 4(29)}}{58} \\approx 5.1926\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8 (condition number)\n",
    "\n",
    "Recall that $\\kappa_{2}(\\boldsymbol{A}) = \\sqrt{\\lambda_{\\max}(\\boldsymbol{A}^{T}\\boldsymbol{A}})/\\sqrt{\\lambda_{\\min}(\\boldsymbol{A}^{T}\\boldsymbol{A}})$. As an approximation we ignore the $(1, 1)$ entry:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\boldsymbol{A}^{T}\\boldsymbol{A} \n",
    "  =\n",
    "  \\begin{bmatrix}\n",
    "     1 & 1 \\\\ 1 & 5\n",
    "  \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Computing the eigenvalues, $\\lambda = 3 \\pm \\sqrt{5}$, therefore $\\kappa_{2} = \\sqrt{3 + \\sqrt{5}}/\\sqrt{3 - \\sqrt{5}} \\approx 2.618$. This is very well conditioned matrix, however LU\n",
    "factorisation would require pivoting. This is an issue with LU factorisation rather than a pathological problem with the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9 (least squares)\n",
    "\n",
    "Define matrix $\\boldsymbol{A}$ and RHS vector $\\boldsymbol{b}$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A matrix: \n",
      " [[1 2]\n",
      " [2 2]\n",
      " [2 0]]\n",
      "b vector: \n",
      " [[5]\n",
      " [6]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2], [2, 2], [2, 0]])\n",
    "print(\"A matrix: \\n\", A)\n",
    "\n",
    "b = np.array([[5], [6], [0]])\n",
    "print(\"b vector: \\n\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Solve least-squares problem using NumPy and print solution $\\boldsymbol{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11111111]\n",
      " [ 2.66666667]]\n"
     ]
    }
   ],
   "source": [
    "x, residual, rank, s = np.linalg.lstsq(A, b)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Compute residual vector: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = A.dot(x) - b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the various norms of the residual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11111111111\n",
      "0.666666666667\n",
      "0.444444444444\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(r, 1))\n",
    "print(np.linalg.norm(r, 2))\n",
    "print(np.linalg.norm(r, np.inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Compute condition number $\\kappa_{2}$ of the normal matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.42013288157\n"
     ]
    }
   ],
   "source": [
    "# Numpy seems to compute the square of the condition number\n",
    "#kappa = np.linalg.cond((A.T).dot(A))\n",
    "#print(kappa)\n",
    " \n",
    "# Compute eigenvalues (returned in ascending order) and print condition number\n",
    "evals = np.linalg.eigvalsh((A.T).dot(A))\n",
    "print(np.sqrt(evals[-1]/evals[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10 (least squares)\n",
    "\n",
    "A projection matrix $\\boldsymbol{P}$ has the property that $\\boldsymbol{P} = \\boldsymbol{P}\\boldsymbol{P}$, and $\\boldsymbol{P}^{T} = \\boldsymbol{P}$.\n",
    " \n",
    "a) The solution to the least squares problem is $\\hat{\\boldsymbol{x}} = \\boldsymbol{A}    \n",
    "   (\\boldsymbol{A}^{T} \\boldsymbol{A})^{-1} \\boldsymbol{A}^{T} \\boldsymbol{b}$.\n",
    "   Therefore $\\boldsymbol{r} = \\boldsymbol{A} \\hat{\\boldsymbol{x}} - \\boldsymbol{b} \n",
    "   = \\boldsymbol{A}(\\boldsymbol{A}^{T} \\boldsymbol{A})^{-1} \\boldsymbol{A}^{T} \n",
    "   \\boldsymbol{b} - \\boldsymbol{b}$. Insert this expression for $\\boldsymbol{r}$ into the expression in the question and re-arrange. to show the result.\n",
    "\n",
    "   Vectors $\\boldsymbol{A}\\boldsymbol{z}$ lie in the column space of $\\boldsymbol{A}$, \n",
    "   hence the expression says that the least-squares residual is orthogonal to the column \n",
    "   space of $\\boldsymbol{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) \n",
    "\\begin{equation}\n",
    "     \\boldsymbol{P}\\boldsymbol{P}\n",
    "        = \\boldsymbol{A} (\\boldsymbol{A}^{T}\\boldsymbol{A})^{-1}\\boldsymbol{A}^{T}\n",
    "    \\boldsymbol{A}(\\boldsymbol{A}^{T}\\boldsymbol{A})^{-1}\\boldsymbol{A}^{T} \n",
    "     = \\boldsymbol{A} (\\boldsymbol{A}^{T}\\boldsymbol{A})^{-1}\\boldsymbol{A}^{T}\n",
    "    = \\boldsymbol{P}\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "        \\boldsymbol{P}^{T}\n",
    "        = \\boldsymbol{A}(\\boldsymbol{A}^{T}\\boldsymbol{A})^{-T}\\boldsymbol{A}^{T}\n",
    "        = \\boldsymbol{A}(\\boldsymbol{A}^{T}\\boldsymbol{A})^{-1}\\boldsymbol{A}^{T}\n",
    "\\end{equation}\n",
    "\n",
    "   by symmetry of $\\boldsymbol{A}^{T}\\boldsymbol{A}$.\n",
    "\n",
    "c)  We can phrase a least squares problem as\n",
    "\n",
    "\\begin{equation}\n",
    "        \\boldsymbol{A} \\hat{\\boldsymbol{x}}\n",
    "        = \\boldsymbol{A}(\\boldsymbol{A}^{T}\\boldsymbol{A})^{-1}\\boldsymbol{A}^{T} \n",
    "        \\boldsymbol{b}\n",
    "        = \\boldsymbol{P} \\boldsymbol{b}\n",
    "\\end{equation}\n",
    "\n",
    "   which says that $\\boldsymbol{P}$ projects $\\boldsymbol{b}$ into the column space \n",
    "   of $\\boldsymbol{A}$. If $\\boldsymbol{b}^{\\prime} = \\boldsymbol{P}\\boldsymbol{b}$ is in the  \n",
    "   column space of $\\boldsymbol{A}$, then $\\boldsymbol{A} \\hat{\\boldsymbol{x}} = \n",
    "   \\boldsymbol{b}^{\\prime}$ has a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11 (pseudo inverse)\n",
    "\n",
    "If the $m \\times n$ matrix $\\boldsymbol{A}$ has linearly independent\n",
    "rows, then the rank of $\\boldsymbol{A}$ is $m$, i.e.~the column space of\n",
    "$\\boldsymbol{A}$ is all of $\\mathbb{R}^{m}$, and left-nullspace space\n",
    "contains the zero vector only.\n",
    "\n",
    "Consider the nullspace of $\\boldsymbol{A}\\boldsymbol{A}^{T}$:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\boldsymbol{A}\\boldsymbol{A}^{T} \\boldsymbol{x} = \\boldsymbol{0} \\ \\rightarrow\n",
    "  \\ \\boldsymbol{x}^{T}\\boldsymbol{A} \\boldsymbol{A}^{T} \\boldsymbol{x} = 0 \\ \\rightarrow\n",
    "  \\ (\\boldsymbol{A}^{T} \\boldsymbol{x})^{T}\\boldsymbol{A}^{T} \\boldsymbol{x} = 0\n",
    "\\end{equation}\n",
    "\n",
    "The above holds only if $\\boldsymbol{A}^{T} \\boldsymbol{x} = \\boldsymbol{0}$, which\n",
    "says that $\\boldsymbol{x}$ must come from the left-nullspace of\n",
    "$\\boldsymbol{A}$.  We have already determined that the left-nullspace of\n",
    "$\\boldsymbol{A}$ contains only the zero vector, therefore\n",
    "$\\boldsymbol{A}\\boldsymbol{A}^{T}$ is full rank (the nullspace of\n",
    "$\\boldsymbol{A}\\boldsymbol{A}^{T}$ contains the zero vector only).\n",
    "\n",
    "Since $\\boldsymbol{A}\\boldsymbol{A}^{T}$ is full rank and can be inverted,\n",
    "\n",
    "\\begin{equation}\n",
    "  \\boldsymbol{A} \\boldsymbol{A}^{+} = \\boldsymbol{A} \\boldsymbol{A}^{T}\n",
    "  (\\boldsymbol{A}\\boldsymbol{A}^{T})^{-1} = \\boldsymbol{I}.\n",
    "\\end{equation}\n",
    "\n",
    "hence $\\boldsymbol{A}^{+}$ is a right-inverse of $\\boldsymbol{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12 (Rayleigh quotient)\n",
    "\n",
    "Define matrix $\\boldsymbol{A}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4 1]\n",
      " [0 0 2]\n",
      " [1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[3, 4, 1], [0, 0, 2], [1, 2, 1]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute eigenvalues directly, then perform power iterations and compute estimate of largest eigenvalue using the Rayleigh quotient at each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact evals: [ 4.24914054  0.85363451 -1.10277505]\n",
      "Eigen estimate: 4.380952380952381\n",
      "Eigen estimate: 4.306930693069307\n",
      "Eigen estimate: 4.252934898612593\n",
      "Eigen estimate: 4.251947132405004\n",
      "Eigen estimate: 4.249174292763802\n",
      "Eigen estimate: 4.249284923084042\n",
      "Eigen estimate: 4.249133833548363\n",
      "Eigen estimate: 4.249148459320735\n",
      "Eigen estimate: 4.249139724124119\n",
      "Eigen estimate: 4.249140998854895\n"
     ]
    }
   ],
   "source": [
    "# Nonsymmetric case\n",
    "evals = np.linalg.eigvals(A)\n",
    "print(\"Exact evals: {}\".format(evals))\n",
    "x0 = np.ones(3)\n",
    "for i in range(10): \n",
    "    x0 = A.dot(x0)\n",
    "    x0/np.linalg.norm(x0, 1)\n",
    "    \n",
    "    eig_est = x0.dot(A.dot(x0))/x0.dot(x0)\n",
    "    print(\"Eigen estimate: {}\".format(eig_est))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make matrix symmetric, and repeat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact evals: [ 4.89218588  0.90825552 -1.8004414 ]\n",
      "Eigen estimate: 4.882352941176471\n",
      "Eigen estimate: 4.891625615763547\n",
      "Eigen estimate: 4.892136681762042\n",
      "Eigen estimate: 4.892180135548904\n",
      "Eigen estimate: 4.892185132691462\n",
      "Eigen estimate: 4.892185778893567\n",
      "Eigen estimate: 4.8921858653608465\n",
      "Eigen estimate: 4.892185877035738\n",
      "Eigen estimate: 4.8921858786157495\n",
      "Eigen estimate: 4.8921858788297055\n"
     ]
    }
   ],
   "source": [
    "# Symmetric case\n",
    "A = 0.5*(A + A.T)\n",
    "evals = np.linalg.eigvals(A)\n",
    "print(\"Exact evals: {}\".format(evals))\n",
    "x0 = np.ones(3)\n",
    "for i in range(10): \n",
    "    x0 = A.dot(x0)\n",
    "    x0/np.linalg.norm(x0, 1)\n",
    "    \n",
    "    eig_est = x0.dot(A.dot(x0))/x0.dot(x0)\n",
    "    print(\"Eigen estimate: {}\".format(eig_est))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the Rayleigh quotient estimate converges much faster for the symmetric matrix. This is because it corresponds to solving a minimisation problem (see lecture notes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13 (stationary iterative methods)\n",
    "\n",
    "Define matrix $\\boldsymbol{A}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2 -1]\n",
      " [-1  2]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[2, -1], [-1, 2]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split $\\boldsymbol{A}$ such that $\\boldsymbol{A} = \\boldsymbol{N} - \\boldsymbol{P}$. A method will converge if the largest eigenvalue of $\\boldsymbol{N}^{-1}\\boldsymbol{P}$ is less the one.\n",
    "\n",
    "For the Richardson method $\\boldsymbol{N} = \\boldsymbol{I}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0. -2.]\n"
     ]
    }
   ],
   "source": [
    "# Richardson\n",
    "N = np.identity(2)\n",
    "P = N - A\n",
    "M = np.linalg.inv(N).dot(P) \n",
    "print(np.linalg.eigvals(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Largest eigenvalue (absolute value) is greater than 1, therefore method will not converge.\n",
    "\n",
    "For the Jacobi method, $\\boldsymbol{N} = \\text{diag}(\\boldsymbol{A})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5 -0.5]\n"
     ]
    }
   ],
   "source": [
    "# Jacobi\n",
    "N = np.diag(np.diag(A))\n",
    "P = N - A\n",
    "M = np.linalg.inv(N).dot(P) \n",
    "print(np.linalg.eigvals(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Largest eigenvalue (absolute value) is less than 1, therefore method will converge.\n",
    "\n",
    "For Gauss-Seidel, $\\boldsymbol{N}$ is the lower triangular part of $\\boldsymbol{A}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.    0.25]\n"
     ]
    }
   ],
   "source": [
    "# Gauss-Seidel\n",
    "N = np.tril(A)\n",
    "P = N - A\n",
    "M = np.linalg.inv(N).dot(P) \n",
    "print(np.linalg.eigvals(M))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gauss-Seidel will converge because largest eigenvalue is less than one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14 (SVD)\n",
    "\n",
    "\\begin{align*}\n",
    "      \\boldsymbol{A}^{-1} &= (\\boldsymbol{U} \\boldsymbol{\\Sigma} \\boldsymbol{V}^{T})^{-1}\n",
    "      \\\\\n",
    "      &=\\boldsymbol{V}^{-T} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{U}^{-1}\n",
    "      \\\\\n",
    "      &=\\boldsymbol{V} \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{U}^{T}\n",
    "\\end{align*}\n",
    "\n",
    "Non-singular matrix cannot have any zero singular value. In fact,\n",
    "smallest singular values is a measure of the `distance' to a\n",
    "singular matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 15 (SVD)\n",
    "\n",
    "Define matrix $\\boldsymbol{A}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3]\n",
      " [2 2]\n",
      " [3 1]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 3], [2, 2], [3, 1]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the reduced SVD (recall that NumPy uses $\\boldsymbol{A} = \\boldsymbol{U} \\boldsymbol{\\Sigma} \\boldsymbol{V}$ rather than  $\\boldsymbol{A} = \\boldsymbol{U} \\boldsymbol{\\Sigma} \\boldsymbol{V}^{T}$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.89897949  2.        ]\n",
      "[[ -5.77350269e-01   7.07106781e-01]\n",
      " [ -5.77350269e-01   1.11022302e-16]\n",
      " [ -5.77350269e-01  -7.07106781e-01]]\n",
      "[[-0.70710678 -0.70710678]\n",
      " [-0.70710678  0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "U, s, V = np.linalg.svd(A, full_matrices=False)\n",
    "print(s)\n",
    "print(U)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute pseudoinverse by creating $\\boldsymbol{\\Sigma}^{+} =  \\boldsymbol{\\Sigma}_{1}^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.16666667  0.08333333  0.33333333]\n",
      " [ 0.33333333  0.08333333 -0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "# Pseudoinverse\n",
    "Sigma_p = np.diag(1.0/s)\n",
    "Ap = (V.T).dot(Sigma_p.dot(U.T))\n",
    "print(Ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $\\boldsymbol{A}^{+}\\boldsymbol{A}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.00000000e+00  -3.88578059e-16]\n",
      " [  2.22044605e-16   1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Check that A^{+}A = I\n",
    "print(Ap.dot(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is the identity. Compute now $\\boldsymbol{A}\\boldsymbol{A}^{+}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.83333333  0.33333333 -0.16666667]\n",
      " [ 0.33333333  0.33333333  0.33333333]\n",
      " [-0.16666667  0.33333333  0.83333333]]\n"
     ]
    }
   ],
   "source": [
    "# Check that AA^{+} \\ne I\n",
    "print(A.dot(Ap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is clearly not the identity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Recall that from $\\boldsymbol{A}^{T} \\boldsymbol{A} \\hat{\\boldsymbol{x}} = \\boldsymbol{A}^{T} \\boldsymbol{b}$ we have \n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\boldsymbol{x}} &= (\\boldsymbol{A}^{T} \\boldsymbol{A})^{-1}\\boldsymbol{A}^{T} \\boldsymbol{b} \\\\\n",
    "&= \\boldsymbol{A}^{+} \\boldsymbol{b}\n",
    "\\end{align}\n",
    "\n",
    "Multiplying both sides by $\\boldsymbol{A}$, \n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{A}\\hat{\\boldsymbol{x}} = \\underbrace{\\boldsymbol{A} \\boldsymbol{A}^{+}}_{\\boldsymbol{P}} \\boldsymbol{b}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\boldsymbol{P}$ is the projection matrix from an earlier questions. Recall that $\\boldsymbol{P}$ projects a vector into the column space of $\\boldsymbol{A}$.\n",
    "Since $\\boldsymbol{P}$ is a projector, it does nothing if $\\boldsymbol{b}$ is already in column space. Therefore any vector $\\boldsymbol{b}$ in column space of $\\boldsymbol{A}$ is a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 16 (pseudo inverse)\n",
    "\n",
    "Define matrix $\\boldsymbol{A}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 3 0]\n",
      " [2 0 0]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[0, 3, 0], [2, 0, 0]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute SVD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.]\n",
      " [ 0.  1.]]\n",
      "[ 3.  2.]\n",
      "[[ 0.  1.  0.]\n",
      " [ 1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "U, s, V = np.linalg.svd(A, full_matrices=False)\n",
    "print(U)\n",
    "print(s)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $\\boldsymbol{\\Sigma}^{+} =  \\boldsymbol{\\Sigma}_{1}^{-1}$, and then $\\boldsymbol{A}^{+}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.5       ]\n",
      " [ 0.33333333  0.        ]\n",
      " [ 0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# \\Sigma^{+}\n",
    "Sigma_p = np.diag(1.0/s)\n",
    "\n",
    "# A^{+}\n",
    "Ap = (V.T).dot(Sigma_p).dot(U)\n",
    "print(Ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $\\boldsymbol{A}^{+}\\boldsymbol{A}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "ApA = Ap.dot(A)\n",
    "print(ApA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this matrix, any $\\boldsymbol{x} = [x_{1} \\ \\ x_{2} \\ \\ 0]$ (which is from the row space of $\\boldsymbol{A}$) satisfies $\\boldsymbol{A}^{+} \\boldsymbol{A} \\boldsymbol{x} = \\boldsymbol{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 17 (rank deficient least squares)\n",
    "\n",
    "Define matrix and RHS vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [1 0 0]\n",
      " [1 1 1]]\n",
      "[0 2 2]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 0, 0], [1, 0, 0], [1, 1, 1]])\n",
    "print(A)\n",
    "\n",
    "b = np.array([0, 2, 2])\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the SVD and print singular values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.00000000e+00   1.00000000e+00   8.16888553e-18]\n"
     ]
    }
   ],
   "source": [
    "# Compute SVD\n",
    "U, s, V = np.linalg.svd(A)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one zero singular value, so we need to 'trim' the last column from $\\boldsymbol{U}$ and the last row from $\\boldsymbol{U}$, and compute $\\boldsymbol{\\sigma}^{+}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.40824829 -0.57735027]\n",
      " [-0.40824829 -0.57735027]\n",
      " [-0.81649658  0.57735027]]\n",
      "[[-0.81649658 -0.40824829 -0.40824829]\n",
      " [-0.57735027  0.57735027  0.57735027]]\n",
      "[[ 0.5  0. ]\n",
      " [ 0.   1. ]]\n"
     ]
    }
   ],
   "source": [
    "# Create view of U with last columns \n",
    "U1 = U[:, :2]\n",
    "print(U1)\n",
    "V1 = V[:2,:]\n",
    "print(V1)\n",
    "\n",
    "# Create Sigma^{+}\n",
    "S1 = np.diag(1.0/s[:-1])\n",
    "print(S1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve problem $\\boldsymbol{x} = \\boldsymbol{V}_{1} \\boldsymbol{\\Sigma}_{1}^{-1} \\boldsymbol{U}_{1}^{T}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.   0.5  0.5]\n"
     ]
    }
   ],
   "source": [
    "x = np.transpose(V1).dot(S1.dot(U1.T).dot(b))\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
